# ğŸ§­ Moral Compass AI

A multi-agent AI that evaluates tough organizational decisions from **legal, ethical, economic, reputational, environmental, public health, and AI risk** perspectives â€” then synthesizes these into a clear recommendation.

---

## ğŸ“œ Overview

**Moral Compass AI** runs multiple specialized â€œagentsâ€ in parallel, each with its own expertise and prompt template. Agents return concise findings, which are merged by a **Synthesis Agent** into a unified summary with confidence scoring. Optionally, a PDF log can be generated.

---

## ğŸ›  Agents

| Agent Name            | Purpose                                                                                                | Example Questions It Handles                                         |
| --------------------- | ------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------- |
| `lawTool`             | Identifies legal risks, compliance issues, labor laws, contracts, notification requirements.           | â€œWhat laws apply if we lay off remote employees in multiple states?â€ |
| `economistTool`       | Economic and financial impact analysis, ROI, cost-benefit tradeoffs.                                   | â€œWill moving production offshore reduce costs in the long run?â€      |
| `prAndReputationTool` | Public relations, brand risk, and stakeholder perception.                                              | â€œWill this decision trigger negative media coverage?â€                |
| `environmentTool`     | Environmental impact, regulatory risks, sustainability alignment.                                      | â€œWhat is the ecological cost of switching to coal power?â€            |
| `deiTool`             | Diversity, equity, and inclusion risks, bias mitigation.                                               | â€œWill this disproportionately impact a protected group?â€             |
| `publicHealthTool`    | Public safety and health implications, relevant public health laws.                                    | â€œWhatâ€™s the risk of reopening schools during an outbreak?â€           |
| `aiRiskTool`          | AI/automation bias, transparency, AI law compliance. Uses **RAG** from Pinecone for grounded evidence. | â€œDoes our chatbot training process risk violating GDPR?â€             |
| `synthesisTool`       | Merges all agent findings into a single recommendation, listing agents used and confidence score.      | N/A â€” always run at the end.                                         |
| `generatePdfLogTool`  | Produces a PDF report of the decision log.                                                             | N/A â€” optional post-synthesis step.                                  |

---

## ğŸ“ˆ Flow

1. **User prompt** â†’ Sent to `/api/decision`.
2. **Moderation & Guardrails**
   - `moderateText()` â€” blocks unsafe prompts.
   - `redactPII()` â€” removes personal identifiers.
3. **Rate limiting**
   - `limitChat()` uses IP-based keys to prevent abuse (configurable in `/lib/rateLimit`).
4. **Tool orchestration** (`streamText`)
   - Relevant agents run in parallel based on prompt content.
   - AI model: `gpt-4.1-nano` (configurable).
5. **Synthesis Agent** combines results.
6. **PDF log** (optional).
7. **Streaming UI** updates agent cards as results arrive.

---

## ğŸ”’ Guardrails

- **Safety moderation** via `moderateText()` before any LLM call.
- **PII redaction** before tool execution.
- **No hallucinated tools** â€” enforced in system prompt.
- **Max step count**: stops after 10 orchestration steps to prevent runaway calls.
- **Output limits**: most tools use `maxOutputTokens: 250` to keep responses concise.

---

## ğŸ“š Retrieval-Augmented Generation (RAG)

The **AI Risk Agent (`aiRiskTool`)** uses RAG with [Pinecone](https://www.pinecone.io/) to ground outputs in stored evidence.

**Process:**

1. `retrieveEvidence(decision, 5)` fetches top-5 semantically relevant chunks.
2. Evidence is injected into the system prompt as a context block.
3. Model **must** base recommendations only on that evidence.
4. This mitigates hallucination and anchors advice in verified sources.

---

## â³ Rate Limits

- Configurable in `/lib/rateLimit`.
- Defaults:
  - **Per-IP limit**: defined by `limitChat` (e.g., X requests per minute/hour).
  - On limit breach â†’ HTTP 429 with `Rate-Limit-*` headers.

---

## ğŸš€ Example Prompt

We plan to replace customer support agents with an AI chatbot trained on customer conversations without explicit consent. How risky is this legally, ethically, and reputationally?

---

## ğŸ“¦ Tech Stack

- **Framework:** Next.js (frontend + backend API routes in a single app)
- **Styling:** Tailwind CSS (utility-first)
- **Orchestration:** `@ai-sdk` for multi-agent streaming
- **LLM Provider:** OpenAI `gpt-4.1-nano` (configurable)
- **Vector Store:** Pinecone (RAG for AI Risk Agent)
- **Rate Limiting:** Upstash Redis (via `/lib/rateLimit`)
- **PDF Generation:** `generatePdfLogTool` for downloadable decision reports

---

## ğŸ“„ License

MIT Â© 2025
